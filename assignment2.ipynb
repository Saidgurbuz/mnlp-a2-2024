{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P0Iyg6btLW9M"
   },
   "source": [
    "#  Assignment 2 - Transfer Learning and Data Augmentation 💬\n",
    "\n",
    "Welcome to the **second assignment** for the **CS-552: Modern NLP course**!\n",
    "\n",
    "> - 😀 Name: **Abdurrahman Said Gürbüz**\n",
    "> - ✉️ Email: **said.gurbuz@epfl.ch**\n",
    "> - 🪪 SCIPER: **369141**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_XjnQhbFIJUu"
   },
   "source": [
    "<div style=\"padding:15px 20px 20px 20px;border-left:3px solid green;background-color:#e4fae4;border-radius: 20px;color:#424242;\">\n",
    "\n",
    "## **Assignment Description**\n",
    "- In the first part of this assignment, you will need to implement training (finetuning) and evaluation of a pre-trained language model ([RoBERTa](https://huggingface.co/docs/transformers/model_doc/roberta)) on a **Sentiment Analysis (SA)** task, which aims to determine whether a product review's emotional tone is positive or negative.\n",
    "\n",
    "- For part-2, following the first finetuning task, you will need to identify the shortcuts (i.e. some salient or toxic features) that the model learnt for the specific task.\n",
    "\n",
    "- For part-3, you are supposed to annotate 80 randomly assigned new datapoints as ground-truth labels. Additionally, the cross annotation should be conducted by another one or two annotators, and you will learn about how to calculate the agreement statistics as a significant characteristic reflecting the quality of a collected dataset.\n",
    "\n",
    "- For part-4, since the human annotation is quite time- and effort-consuming, there are plenty of ways to get silver-labels from automatic labeling to augment the dataset scale, e.g., paraphrasing each text input in different words without changing its meaning. You will use a [T5](https://huggingface.co/docs/transformers/en/model_doc/t5) paraphrase model to expand the training data of sentiment analysis, and evaluate the improvement of data augmentation.\n",
    "\n",
    "For Parts 1 and Part 2, you will need to complete the code in the corresponding `.py` files (`sa.py` for Part 1, `shortcut.py` for Part 2). You will be provided with the function descriptions and detailed instructions about the code snippet you need to write.\n",
    "\n",
    "\n",
    "### Table of Contents\n",
    "- **PART 1: Sentiment Analysis (33 pts)**\n",
    "    - 1.1 Dataset Processing (10 pts)\n",
    "    - 1.2 Model Training and Evaluation (18 pts)\n",
    "    - 1.3 Fine-Grained Validation (5 pts)\n",
    "- **PART 2: Identify Model Shortcuts (22 pts)**\n",
    "    - 2.1 N-gram Pattern Extraction (6 pts)\n",
    "    - 2.2 Distill Potentially Useful Patterns (8 pts)\n",
    "    - 2.3 Case Study (8 pts)\n",
    "- **PART 3: Annotate New Data (25 pts)**\n",
    "    - 3.1 Write an Annotation Guideline (5 pts)\n",
    "    - 3.2 Annotate Your Datapoints with Partner(s) (8 pts)\n",
    "    - 3.3 Agreement Measure (12 pts)\n",
    "- **PART 4: Data Augmentation (20 pts)**\n",
    "    - 4.1 Data Augmentation with Paraphrasing (15 pts)\n",
    "    - 4.2 Retrain RoBERTa Model with Data Augmentation (5 pts)\n",
    "    \n",
    "### Deliverables\n",
    "\n",
    "- ✅ This jupyter notebook: `assignment2.ipynb`\n",
    "- ✅ `sa.py` and `shortcut.py` file\n",
    "- ✅ Checkpoints for RoBERTa models finetuned on original and augmented SA training data (Part 1 and Part 4), including:\n",
    "    - `models/lr1e-05-warmup0.3/`\n",
    "    - `models/lr2e-05-warmup0.3/`\n",
    "    - `models/augmented/lr1e-05-warmup0.3/`\n",
    "- ✅ Model prediction results on each domain data (Part 1.3 Fine-Grained Validation): `predictions/`\n",
    "- ✅ Cross-annotated new SA data (Part 3), including:\n",
    "    - `data/<your_assigned_dataset_id>-<your_sciper_number>.jsonl`\n",
    "    - `data/<your_assigned_dataset_id>-<your_partner_sciper_number>.jsonl`\n",
    "    - (for group of 3) `data/<your_assigned_dataset_id>-<your_second_partner_sciper_number>.jsonl`\n",
    "- ✅ Paraphrase-augmented SA training data (Part 4), including:\n",
    "    - `data/augmented_train_sa.jsonl`\n",
    "- ✅ `./tensorboard` directory with logs for all trained/finetuned models, including:\n",
    "    - `tensorboard/part1_lr1e-05/`\n",
    "    - `tensorboard/part1_lr2e-05/`\n",
    "    - `tensorboard/part4_lr1e-05/`\n",
    "\n",
    "### How to implement this assignment\n",
    "\n",
    "Please read carefully the following points. All the information on how to read, implement and submit your assignment is explained in details below:\n",
    "\n",
    "1. For this assignment, you will need to implement and fill in the missing code snippets for both the **Jupyter Notebook `assignment2.ipynb`** and the **`sa.py`**, **`shortcut.py`** python files.\n",
    "\n",
    "2. Along with above files, you need to additionally upload model files under the **`models/`** dir, regarding the following models:\n",
    "    - finetuned RoBERTa models on original SA training data (PART 1)  \n",
    "    - finetuned RoBERTa model on augmented SA training data (PART 4)\n",
    "\n",
    "3. You also need to upload model prediction results in Part 1.3 Fine-Grained Validation, saved in **`predictions/`**.\n",
    "\n",
    "4. You also need to upload new data files under the **`data/`** dir (along with our already provided data), including:\n",
    "    - new SA data with your and your partner's annotations (Part 3)\n",
    "    - paraphrase-augmented SA training data (Part 4)\n",
    "\n",
    "5. Finally, you will need to log your training using Tensorboard. Please follow the instructions in the `README.md` of the **``tensorboard/``** directory.\n",
    "\n",
    "**Note**: Large files such as model checkpoints and logs should be pushed to the repository with Git LFS. You may also find that training the models on a GPU can speed up the process, we recommend using Colab's free GPU service for this. A tutorial on how to use Git LFS and Colab can be found [here](https://github.com/epfl-nlp/cs-552-modern-nlp/blob/main/Exercises/tutorials.md).\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9iJ_9sA2KyP2"
   },
   "source": [
    "<div style=\"padding:15px 20px 20px 20px;border-left:3px solid orange;background-color:#fff5d6;border-radius: 20px;color:#424242;\">\n",
    "\n",
    "## **Environment Setup**\n",
    "\n",
    "### **Option 1: creating your own environment**\n",
    "\n",
    "```\n",
    "conda create --name mnlp-a2 python=3.10\n",
    "conda activate mnlp-a2\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "**Note**: If some package versions in our suggested environment do not work, feel free to try other package versions suitable for your computer, but remember to update ``requirements.txt`` and explain the environment changes in your notebook (no penalty for this if necessary).\n",
    "\n",
    "### **Option 2: using Google Colab**\n",
    "If you are using Google Colab notebook for this assignment, you will need to run a few commands to set up our environment on Google Colab, as shown below:\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell makes sure modules are auto-loaded when you change external python files\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VfVHqiSvK1aB"
   },
   "outputs": [],
   "source": [
    "# If you are working in Colab, then consider mounting your assignment folder to your drive\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "# Direct to your assignment folder.\n",
    "#%cd /content/drive/MyDrive/path-to-your-assignment-folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2LcGSuvWtmPf"
   },
   "source": [
    "Install packages that are not included in the Colab base envrionemnt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "SFXMx5FXtZhQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.1.0 in /Users/saidgurbuz/miniconda3/envs/mnlp-a2/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: transformers==4.38.1 in /Users/saidgurbuz/miniconda3/envs/mnlp-a2/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (4.38.1)\n",
      "Requirement already satisfied: nltk==3.8.1 in /Users/saidgurbuz/miniconda3/envs/mnlp-a2/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (3.8.1)\n",
      "Requirement already satisfied: scikit-learn==1.2.2 in /Users/saidgurbuz/miniconda3/envs/mnlp-a2/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (1.2.2)\n",
      "Requirement already satisfied: huggingface-hub==0.20.3 in /Users/saidgurbuz/miniconda3/envs/mnlp-a2/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (0.20.3)\n",
      "Requirement already satisfied: numpy==1.25.2 in /Users/saidgurbuz/miniconda3/envs/mnlp-a2/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (1.25.2)\n",
      "Requirement already satisfied: tqdm==4.66.2 in /Users/saidgurbuz/miniconda3/envs/mnlp-a2/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (4.66.2)\n",
      "Requirement already satisfied: scipy==1.11.4 in /Users/saidgurbuz/miniconda3/envs/mnlp-a2/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (1.11.4)\n",
      "Requirement already satisfied: urllib3==2.0.7 in /Users/saidgurbuz/miniconda3/envs/mnlp-a2/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (2.0.7)\n",
      "Requirement already satisfied: six==1.16.0 in /Users/saidgurbuz/miniconda3/envs/mnlp-a2/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (1.16.0)\n",
      "Requirement already satisfied: tensorboard==2.15.2 in /Users/saidgurbuz/miniconda3/envs/mnlp-a2/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (2.15.2)\n",
      "Requirement already satisfied: jsonlines==4.0.0 in /Users/saidgurbuz/miniconda3/envs/mnlp-a2/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (4.0.0)\n",
      "Requirement already satisfied: filelock in /Users/saidgurbuz/miniconda3/envs/mnlp-a2/lib/python3.10/site-packages (from torch==2.1.0->-r requirements.txt (line 1)) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/saidgurbuz/miniconda3/envs/mnlp-a2/lib/python3.10/site-packages (from torch==2.1.0->-r requirements.txt (line 1)) (4.10.0)\n",
      "Requirement already satisfied: sympy in /Users/saidgurbuz/miniconda3/envs/mnlp-a2/lib/python3.10/site-packages (from torch==2.1.0->-r requirements.txt (line 1)) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/saidgurbuz/miniconda3/envs/mnlp-a2/lib/python3.10/site-packages (from torch==2.1.0->-r requirements.txt (line 1)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/saidgurbuz/miniconda3/envs/mnlp-a2/lib/python3.10/site-packages (from torch==2.1.0->-r requirements.txt (line 1)) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/saidgurbuz/miniconda3/envs/mnlp-a2/lib/python3.10/site-packages (from torch==2.1.0->-r requirements.txt (line 1)) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/saidgurbuz/miniconda3/envs/mnlp-a2/lib/python3.10/site-packages (from transformers==4.38.1->-r requirements.txt (line 2)) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/saidgurbuz/miniconda3/envs/mnlp-a2/lib/python3.10/site-packages (from transformers==4.38.1->-r requirements.txt (line 2)) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/saidgurbuz/miniconda3/envs/mnlp-a2/lib/python3.10/site-packages (from transformers==4.38.1->-r requirements.txt (line 2)) (2023.12.25)\n",
      "Requirement already satisfied: requests in /Users/saidgurbuz/miniconda3/envs/mnlp-a2/lib/python3.10/site-packages (from transformers==4.38.1->-r requirements.txt (line 2)) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/saidgurbuz/miniconda3/envs/mnlp-a2/lib/python3.10/site-packages (from transformers==4.38.1->-r requirements.txt (line 2)) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/saidgurbuz/miniconda3/envs/mnlp-a2/lib/python3.10/site-packages (from transformers==4.38.1->-r requirements.txt (line 2)) (0.4.2)\n",
      "Requirement already satisfied: click in /Users/saidgurbuz/miniconda3/envs/mnlp-a2/lib/python3.10/site-packages (from nltk==3.8.1->-r requirements.txt (line 3)) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/saidgurbuz/miniconda3/envs/mnlp-a2/lib/python3.10/site-packages (from nltk==3.8.1->-r requirements.txt (line 3)) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/saidgurbuz/miniconda3/envs/mnlp-a2/lib/python3.10/site-packages (from scikit-learn==1.2.2->-r requirements.txt (line 4)) (3.4.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/saidgurbuz/miniconda3/envs/mnlp-a2/lib/python3.10/site-packages (from tensorboard==2.15.2->-r requirements.txt (line 11)) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /Users/saidgurbuz/miniconda3/envs/mnlp-a2/lib/python3.10/site-packages (from tensorboard==2.15.2->-r requirements.txt (line 11)) (1.62.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/saidgurbuz/miniconda3/envs/mnlp-a2/lib/python3.10/site-packages (from tensorboard==2.15.2->-r requirements.txt (line 11)) (2.29.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /Users/saidgurbuz/miniconda3/envs/mnlp-a2/lib/python3.10/site-packages (from tensorboard==2.15.2->-r requirements.txt (line 11)) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/saidgurbuz/miniconda3/envs/mnlp-a2/lib/python3.10/site-packages (from tensorboard==2.15.2->-r requirements.txt (line 11)) (3.6)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /Users/saidgurbuz/miniconda3/envs/mnlp-a2/lib/python3.10/site-packages (from tensorboard==2.15.2->-r requirements.txt (line 11)) (4.25.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/saidgurbuz/miniconda3/envs/mnlp-a2/lib/python3.10/site-packages (from tensorboard==2.15.2->-r requirements.txt (line 11)) (68.2.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/saidgurbuz/miniconda3/envs/mnlp-a2/lib/python3.10/site-packages (from tensorboard==2.15.2->-r requirements.txt (line 11)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/saidgurbuz/miniconda3/envs/mnlp-a2/lib/python3.10/site-packages (from tensorboard==2.15.2->-r requirements.txt (line 11)) (3.0.1)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /Users/saidgurbuz/miniconda3/envs/mnlp-a2/lib/python3.10/site-packages (from jsonlines==4.0.0->-r requirements.txt (line 12)) (23.2.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/saidgurbuz/miniconda3/envs/mnlp-a2/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard==2.15.2->-r requirements.txt (line 11)) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/saidgurbuz/miniconda3/envs/mnlp-a2/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard==2.15.2->-r requirements.txt (line 11)) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/saidgurbuz/miniconda3/envs/mnlp-a2/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard==2.15.2->-r requirements.txt (line 11)) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/saidgurbuz/miniconda3/envs/mnlp-a2/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard==2.15.2->-r requirements.txt (line 11)) (1.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/saidgurbuz/miniconda3/envs/mnlp-a2/lib/python3.10/site-packages (from requests->transformers==4.38.1->-r requirements.txt (line 2)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/saidgurbuz/miniconda3/envs/mnlp-a2/lib/python3.10/site-packages (from requests->transformers==4.38.1->-r requirements.txt (line 2)) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/saidgurbuz/miniconda3/envs/mnlp-a2/lib/python3.10/site-packages (from requests->transformers==4.38.1->-r requirements.txt (line 2)) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/saidgurbuz/miniconda3/envs/mnlp-a2/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard==2.15.2->-r requirements.txt (line 11)) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/saidgurbuz/miniconda3/envs/mnlp-a2/lib/python3.10/site-packages (from sympy->torch==2.1.0->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Users/saidgurbuz/miniconda3/envs/mnlp-a2/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard==2.15.2->-r requirements.txt (line 11)) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/saidgurbuz/miniconda3/envs/mnlp-a2/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard==2.15.2->-r requirements.txt (line 11)) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" # limiting to one GPU\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saidgurbuz/miniconda3/envs/mnlp-a2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x156dc4c50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import jsonlines\n",
    "import random\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "# TODO: Enter your Sciper number\n",
    "SCIPER = '369141'\n",
    "seed = int(SCIPER)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS is enabled!\n"
     ]
    }
   ],
   "source": [
    "# Check the availability of GPU (proceed only it returns True!)\n",
    "if torch.cuda.is_available():\n",
    "  print('Good to go!')\n",
    "elif torch.backends.mps.is_available():\n",
    "  print('MPS is enabled!')\n",
    "else:\n",
    "  print('Please set GPU via Edit -> Notebook Settings.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHhgkhaH-IUl"
   },
   "source": [
    "<div style=\"padding:15px 20px 20px 20px;border-left:3px solid orange;background-color:#fff5d6;border-radius: 20px;color:#424242;\">\n",
    "    \n",
    "# PART 1: Sentiment Analysis (33 pts)\n",
    "\n",
    "In this part, we will finetune a pretrained language model (Roberta) on sentiment analysis(SA) task. \n",
    "\n",
    "> Specifically, we will focus on a binary sentiment classification task for multi-domain product reviews. It requires the model to **classify a given paragraph of review by its sentiment polarity (positive or negative)**. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tD2YPuqeIYBN"
   },
   "source": [
    "### Load Training Dataset (`train_sa.jsonl`) \n",
    "\n",
    "**You can run the following cell to have the first glance at your data**. Each data sample is a python dictionary, which consists of following components:\n",
    "- input review (*'review'*): a natural language sentence or a paragraph commenting about a product.\n",
    "- domain (*'domain'*): describing the type of product being reviewed.\n",
    "- label of sentiment (*'label'*): indicating whether the review states positive or negative views about the product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "p-ODgcNUqYtm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'review': \"THis book was horrible.  If it was possible to rate it lower than one star i would have.  I am an avid reader and picked this book up after my mom had gotten it from a friend.  I read half of it, suffering from a headache the entire time, and then got to the part about the relationship the 13 year old boy had with a 33 year old man and i lit this book on fire.  One less copy in the world...don't waste your money. I wish i had the time spent reading this book back so i could use it for better purposes.  THis book wasted my life\", 'domain': 'books', 'label': 'negative'}\n",
      "{'review': 'Sphere by Michael Crichton is an excellant novel. This was certainly the hardest to put down of all of the Crichton novels that I have read. The story revolves around a man named Norman Johnson. Johnson is a phycologist. He travels with 4 other civilans to a remote location in the Pacific Ocean to help the Navy in a top secret misssion. They quickly learn that under the ocean is a half mile long spaceship. The civilans travel to a center 1000 feet under the ocean to live while researching the spacecraft. They are joined by 5 Navy personel to help them run operations. However on the surface a typhoon comes and the support ships on the surface must leave. The team of ten is stuck 1000 feet under the surface of the ocean. After a day under the sea they find out that the spacecraft is actually an American ship that has explored black holes and has brought back some strange things back to earth. This novel does not have the research that some of the other Crichton novels have, but it still has a lot of information on random things from the lawes of partial pressure to behavior analysis. I would strongly recommend this book', 'domain': 'books', 'label': 'positive'}\n",
      "{'review': \"This entire movie could have run in only 20 minutes and you wouldn't miss anything and might even enjoy it. Unfortunately it ran 88 minutes too long and I couldn't wait for it to end.  I saw it in the theater and the people all around me were all complaining how boring it was. At least a quarter of them walked out before the end. It's that bad. It's a shame, I love a good suspense/horror movie and the decent actors in this movies were waisted\", 'domain': 'dvd', 'label': 'negative'}\n",
      "{'review': \"I'm not sure why Sony, which now owns I Dream of Jeannie, decided to colorize the first season of this series.  Whatever the reason, you can readily tell by looking at the prices here on Amazon.com that the original black-and-white version of the first season is worth a lot more.  The reason for that is simple--I Dream of Jeannie was originally broadcast in black-and-white.  And for a television fan like myself, that's the ONLY way to watch the first season. The episodes themselves are just as I remember seeing them.  Since I wasn't around in 1965, I'm pretty sure I've never seen these without the cuts that have been referenced here.  But to me, they're still pretty good.  The theme music, in my opinion, is every bit as good as the second theme, introduced when Jeannie went to color in 1966. The one thing that truly will drive the purists nuts is the fact that Sony stripped off the old Screen Gems animation from the end of every episode.  That logo was attached to so many classic shows from the 1960s and 1970s, and it is consistenly rated, along with Viacom's old blue V of Doom, as the scariest logo in the history of television.  The new Sony outro doesn't pack the same punch. Still, if you liked Jeannie way back when, you'll love it now, especially since you can watch it anytime you like, without commercial interruption\", 'domain': 'dvd', 'label': 'positive'}\n",
      "{'review': 'cons tips extremely easy on carpet and if you have a lot of cds stacked at the top poorly designed, it is a vertical cd rack that doesnt have individual slots for cds, so if you want a cd from the bottom of a stack you have basically pull the whole stack to get to it putting it together was a pain, the one i bought i had to break a piece of metal just to fit it in its guide holes. again..poorly designed... doesnt even fit cds that well, there are gaps, and the cd casses are loose fitting pros .......... i guess it can hold a lot of cds....', 'domain': 'electronics', 'label': 'negative'}\n",
      "{'review': 'I purchased this unit due to frequent blackouts in my area and 2 power supplies going bad.  It will run my cable modem, router, PC, and LCD monitor for 5 minutes.  This is more than enough time to save work and shut down.   Equally important, I know that my electronics are receiving clean power. I feel that this investment is minor compared to the loss of valuable data or the failure of equipment due to a power spike or an irregular power supply. As always, Amazon had it to me in &lt;2 business days', 'domain': 'electronics', 'label': 'positive'}\n",
      "{'review': \"He just looks away from where the spray emits--and barks again! It also doesn't work 100% of the time...and we're not sure why.  When we fill it, it seems to work fairly well right after but it either does not have as many sprays as it is supposed to, or it isn't working very long. It does work well for my other small dog who is not such a persistent barker.  Terriers are just too stubborn to care if they're getting sprayed, I guess.\", 'domain': 'housewares', 'label': 'negative'}\n",
      "{'review': 'For those of you unfamiliar with the \"A Series of Unfortunate Events\" books, they detail the absurdly tragic lives of the fictional Baudelaire orphans, and their struggles to overcome adversity (after adversity, after adversity, ad infinitum).  Hovering in the background of their tragedies and occasional, brief glimpses of happiness, is their distant cousin, the scheming, greedy, conniving, ruthless cousin, the Count. Given the concept of the story, this poster fits perfectly, with the orphans, standing together but otherwise alone, in the middle, with the shadow of the Count casting gloom upon them.  It is a perfect fit, as is the casting of Jim Carrey as the Count', 'domain': 'housewares', 'label': 'positive'}\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'data'\n",
    "data_train_path = os.path.join(data_dir, 'train_sa.jsonl')\n",
    "with jsonlines.open(data_train_path, \"r\") as reader:\n",
    "    for sid, sample in enumerate(reader.iter()):\n",
    "        if sid % 200 == 0:\n",
    "            print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "BvM8jd_3QObg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 25.0/25.0 [00:00<00:00, 91.9kB/s]\n",
      "vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 3.27MB/s]\n",
      "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 1.61MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 12.2MB/s]\n",
      "config.json: 100%|██████████| 481/481 [00:00<00:00, 4.50MB/s]\n",
      "model.safetensors: 100%|██████████| 499M/499M [00:20<00:00, 24.3MB/s] \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# We use the following pretrained tokenizer and model\n",
    "model_name = \"FacebookAI/roberta-base\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fCETOFT2dB4u"
   },
   "source": [
    "## 🎯 Q1.1: **Dataset Processing (10 pts)**\n",
    "\n",
    "Our first step is to constructing a Pytorch Dataset for SA task. Specifically, we will need to implement **tokenization** and **padding** using a HuggingFace pre-trained tokenizer.\n",
    "\n",
    "**TODO🔻: Complete `SADataset` class following the instructions in `sa.py`, and test by running the following cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "_5Sya9W5BTDl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building SA Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1600it [00:00, 2207.36it/s]\n"
     ]
    }
   ],
   "source": [
    "from sa import SADataset\n",
    "model_name = \"FacebookAI/roberta-base\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "dataset = SADataset(\"data/train_sa.jsonl\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SADataset test correct ✅\n"
     ]
    }
   ],
   "source": [
    "from testA2 import test_SADataset\n",
    "test_SADataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0weQpG6_3vO"
   },
   "source": [
    "## 🎯 Q1.2: **Model Training and Evaluation (18 pts)**\n",
    "\n",
    "Next, we will implement the training and evaluation process to finetune the model. \n",
    "\n",
    "- For training: you will need to calculate the **loss** and update the model weights by using **Adam optimizer**. Additionally, we add a **learning rate schedular** to adopt an adaptive learning rate during the whole training process.\n",
    "\n",
    "- For evaluation: you will need to compute the **confusion matrix** and **F1 scores** to assess the model performance.\n",
    "\n",
    "**TODO🔻: Complete the `compute_metrics()`, `train()` and `evaluate()` functions following the instructions in the `sa.py` file, you can test compute_metrics() by running the following cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6w7Leraw4tIY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute_metric test correct ✅\n"
     ]
    }
   ],
   "source": [
    "from sa import compute_metrics, train, evaluate\n",
    "\n",
    "from testA2 import test_compute_metrics\n",
    "test_compute_metrics(compute_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvCUS748_3vS"
   },
   "source": [
    "#### **Start Training and Validation!**\n",
    "\n",
    "TODO🔻: (1) [coding question] Train the model with the following two different learning rates (other hyperparameters should be kept consistent). \n",
    "\n",
    "> A. learning_rate = 1e-5\n",
    "\n",
    "> B. learning_rate = 2e-5\n",
    "\n",
    "**Note:** *Each training will take ~7-10 minutes using a T4 Colab GPU.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "zn66mMOj_3vS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.mps.manual_seed(seed)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "\n",
    "model_name = \"FacebookAI/roberta-base\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "batch_size = 8\n",
    "epochs = 4\n",
    "max_grad_norm = 1.0\n",
    "warmup_percent = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Z5aWqR1h_3vS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building SA Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1600it [00:00, 2223.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building SA Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6400it [00:02, 2688.61it/s]\n",
      "/Users/saidgurbuz/miniconda3/envs/mnlp-a2/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Training:   0%|          | 0/200 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "Training: 100%|██████████| 200/200 [00:58<00:00,  3.41it/s]\n",
      "Evaluation: 100%|██████████| 800/800 [00:50<00:00, 15.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Training Loss: 0.654 | Validation Loss: 0.392\n",
      "Epoch 0 SA Validation:\n",
      "Confusion Matrix:\n",
      "[[2546  654]\n",
      " [ 197 3003]]\n",
      "F1: (85.68%, 87.59%) | Macro-F1: 86.63%\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 200/200 [00:54<00:00,  3.70it/s]\n",
      "Evaluation: 100%|██████████| 800/800 [00:48<00:00, 16.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Training Loss: 0.339 | Validation Loss: 0.463\n",
      "Epoch 1 SA Validation:\n",
      "Confusion Matrix:\n",
      "[[3059  141]\n",
      " [ 580 2620]]\n",
      "F1: (89.46%, 87.90%) | Macro-F1: 88.68%\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 200/200 [00:54<00:00,  3.69it/s]\n",
      "Evaluation: 100%|██████████| 800/800 [00:48<00:00, 16.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Training Loss: 0.228 | Validation Loss: 0.406\n",
      "Epoch 2 SA Validation:\n",
      "Confusion Matrix:\n",
      "[[2823  377]\n",
      " [ 233 2967]]\n",
      "F1: (90.25%, 90.68%) | Macro-F1: 90.46%\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 200/200 [00:54<00:00,  3.64it/s]\n",
      "Evaluation: 100%|██████████| 800/800 [00:48<00:00, 16.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Training Loss: 0.126 | Validation Loss: 0.513\n",
      "Epoch 3 SA Validation:\n",
      "Confusion Matrix:\n",
      "[[2920  280]\n",
      " [ 294 2906]]\n",
      "F1: (91.05%, 91.01%) | Macro-F1: 91.03%\n",
      "Model Saved!\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-5  # play around with this hyperparameter\n",
    "\n",
    "train_dataset = SADataset(\"data/train_sa.jsonl\", tokenizer)\n",
    "test_dataset = SADataset(\"data/test_sa.jsonl\", tokenizer)\n",
    "\n",
    "train(train_dataset, test_dataset, model, device, batch_size=batch_size, epochs=epochs, max_grad_norm=max_grad_norm, warmup_percent=warmup_percent, learning_rate=learning_rate,\n",
    "      model_save_root='models/', tensorboard_path=\"./tensorboard/part1_lr{}\".format(learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building SA Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1600it [00:00, 2183.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building SA Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6400it [00:02, 2670.28it/s]\n",
      "Training: 100%|██████████| 200/200 [00:59<00:00,  3.39it/s]\n",
      "Evaluation: 100%|██████████| 800/800 [00:49<00:00, 16.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Training Loss: 0.545 | Validation Loss: 0.322\n",
      "Epoch 0 SA Validation:\n",
      "Confusion Matrix:\n",
      "[[2930  270]\n",
      " [ 449 2751]]\n",
      "F1: (89.07%, 88.44%) | Macro-F1: 88.76%\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 200/200 [00:54<00:00,  3.64it/s]\n",
      "Evaluation: 100%|██████████| 800/800 [00:48<00:00, 16.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Training Loss: 0.332 | Validation Loss: 0.365\n",
      "Epoch 1 SA Validation:\n",
      "Confusion Matrix:\n",
      "[[2962  238]\n",
      " [ 375 2825]]\n",
      "F1: (90.62%, 90.21%) | Macro-F1: 90.42%\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 200/200 [00:55<00:00,  3.63it/s]\n",
      "Evaluation: 100%|██████████| 800/800 [00:48<00:00, 16.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Training Loss: 0.169 | Validation Loss: 0.545\n",
      "Epoch 2 SA Validation:\n",
      "Confusion Matrix:\n",
      "[[3043  157]\n",
      " [ 584 2616]]\n",
      "F1: (89.15%, 87.59%) | Macro-F1: 88.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 200/200 [00:53<00:00,  3.71it/s]\n",
      "Evaluation: 100%|██████████| 800/800 [00:48<00:00, 16.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Training Loss: 0.095 | Validation Loss: 0.523\n",
      "Epoch 3 SA Validation:\n",
      "Confusion Matrix:\n",
      "[[2987  213]\n",
      " [ 451 2749]]\n",
      "F1: (90.00%, 89.22%) | Macro-F1: 89.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# For 2e-5 learning rate\n",
    "learning_rate = 2e-5\n",
    "model_name = \"FacebookAI/roberta-base\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "train_dataset = SADataset(\"data/train_sa.jsonl\", tokenizer)\n",
    "test_dataset = SADataset(\"data/test_sa.jsonl\", tokenizer)\n",
    "\n",
    "train(train_dataset, test_dataset, model, device, batch_size=batch_size, epochs=epochs, max_grad_norm=max_grad_norm, warmup_percent=warmup_percent, learning_rate=learning_rate,\n",
    "      model_save_root='models/', tensorboard_path=\"./tensorboard/part1_lr{}\".format(learning_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO🔻: (2) [textual question] compare and discuss the results. \n",
    "\n",
    "- Which learning rate is better? Explain your answers.\n",
    "\n",
    "- Answer: Although the training loss become smaller in learning rate 2e-5, with learning rate 1e-5, the model achieved a higher F1 scores of ~0.91, while with learning rate 2e-5, we get macro-F1 score of ~0.895. This shows that the model overfit a bit with learning rate 2e-5. However, both learning rates converge to decent f1 scores which can be due to the learning rate scheduling.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGuzGJCB_3vT"
   },
   "source": [
    "## 🎯 Q1.3: **Fine-Grained Validation (5 pts)**\n",
    "\n",
    "TODO🔻: (1) [coding question] Use the model checkpoint trained from the first learning_rate setting (lr=1e-5), check the model performance on each domain subsets of the validation set. You should report **the validation loss**, **confusion matrix**, **F1 scores** and **Macro-F1 on each domain**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "YCWWJjTP_3vT"
   },
   "outputs": [],
   "source": [
    "# Split the test sets into subsets with different domains\n",
    "# Save the subsets under 'data/'\n",
    "# Replace \"...\" with your code\n",
    "domain_data = {}\n",
    "with jsonlines.open(\"data/test_sa.jsonl\", mode=\"r\") as reader:\n",
    "    for sample in reader:\n",
    "        domain = sample[\"domain\"]\n",
    "        if domain not in domain_data:\n",
    "            domain_data[domain] = []\n",
    "        domain_data[domain].append(sample)\n",
    "\n",
    "for domain, samples in domain_data.items():\n",
    "    with jsonlines.open(\"data/test_sa_\"+domain+\".jsonl\", mode=\"w\") as writer:\n",
    "        for sd in samples:\n",
    "            writer.write(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"models/lr1e-05-warmup0.3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "Q4J2pu60xHTd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building SA Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1600it [00:00, 1638.81it/s]\n",
      "Evaluation: 100%|██████████| 200/200 [00:51<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain: books\n",
      "Validation Loss: 0.509\n",
      "Confusion Matrix:\n",
      "[[735  65]\n",
      " [ 84 716]]\n",
      "F1: (90.80%, 90.58%) | Macro-F1: 90.69%\n",
      "Building SA Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1600it [00:00, 2051.59it/s]\n",
      "Evaluation: 100%|██████████| 200/200 [00:52<00:00,  3.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain: dvd\n",
      "Validation Loss: 0.621\n",
      "Confusion Matrix:\n",
      "[[721  79]\n",
      " [ 86 714]]\n",
      "F1: (89.73%, 89.64%) | Macro-F1: 89.69%\n",
      "Building SA Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1600it [00:00, 3256.27it/s]\n",
      "Evaluation: 100%|██████████| 200/200 [00:54<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain: electronics\n",
      "Validation Loss: 0.497\n",
      "Confusion Matrix:\n",
      "[[727  73]\n",
      " [ 69 731]]\n",
      "F1: (91.10%, 91.15%) | Macro-F1: 91.12%\n",
      "Building SA Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1600it [00:00, 4173.76it/s]\n",
      "Evaluation: 100%|██████████| 200/200 [00:54<00:00,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain: housewares\n",
      "Validation Loss: 0.424\n",
      "Confusion Matrix:\n",
      "[[737  63]\n",
      " [ 55 745]]\n",
      "F1: (92.59%, 92.66%) | Macro-F1: 92.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-5\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"models/lr1e-05-warmup0.3\")\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"models/lr1e-05-warmup0.3\")\n",
    "model.to(device)\n",
    "\n",
    "results_save_dir = 'predictions/'\n",
    "\n",
    "# Evaluate and save prediction results in each domain\n",
    "# Replace \"...\" with your code\n",
    "for domain in [domain for domain in domain_data.keys()]:\n",
    "    test_dataset = SADataset(\"data/test_sa_\"+domain+\".jsonl\", tokenizer)\n",
    "    dev_loss, confusion, f1_pos, f1_neg = evaluate(test_dataset, model, device, batch_size=batch_size,\n",
    "                                                   result_save_file='predictions/test_'+domain+'.jsonl')\n",
    "    macro_f1 = (f1_pos + f1_neg) / 2\n",
    "\n",
    "    print(f'Domain: {domain}')\n",
    "    print(f'Validation Loss: {dev_loss:.3f}')\n",
    "    print(f'Confusion Matrix:')\n",
    "    print(confusion)\n",
    "    print(f'F1: ({f1_pos*100:.2f}%, {f1_neg*100:.2f}%) | Macro-F1: {macro_f1*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO🔻: (2) [textual question] compare and discuss the results. \n",
    "\n",
    "**Questions:**\n",
    "- On which domain does the model perform the best? the worst?\n",
    "- Give some possible explanations of why the model's best-performed domain is easier, and why the model's worst-performed domain is more challenging. Use some examples to support your explanations.\n",
    "\n",
    "**Note:** To find examples for supporting your discussion, save the model prediction results on each domain under the `predictions/` folder, by specifying the `result_save_file` parameter in the *evaluate* function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p8j7l7mrS2iH"
   },
   "source": [
    "\n",
    "(Write your answer to the questions here.) \\\n",
    "**Answers:**\n",
    "- The model performed best on the 'housewares' domain with a macro-F1 score of ~0.925, and the worst on the 'dvd' domain with a macro-F1 score of ~0.897. \n",
    "- One of the reason for this difference is, The reviews in the 'housewares' domain is easier for the model to predict because they are in general very straightforward and less ambiguous (e.g. predictions/test_housewares.jsonl, line-14 *\"Easily the worst toaster ever. ... I'd give it a minus 5 stars, but they do not even have a 0 star vote\"*). On the other hand, the 'dvd' domain is more challenging because the reviews in this domain are longer, more ambiguous and contain more complicated statements (as a movie can contains too many positive and negative aspects together). For instance (here I copy past whole review to better explain the complexity), predictions/test_dvd.jsonl, line-3: *\"I've been waiting for this season, literally, for years.  It contained (notice the past tense) the funniest scene ever in the entire run of the show.  It was in the emergency clinic in the episode \\\"My Sister, My Sitter\\\" where the scene pans around showing the patients.  Smithers appears holding an empty gerbil cage and is the only patient not sitting down.  When I first saw that, many years ago, I literally rolled off of the couch with laughter as my wife stared at me like I needed a padded room.  She didn't even understand the joke when I explained it as an alternate lifestyle practice of \\\"hiding\\\" the rodent so as to make sitting down more than a little uncomfortable.  This DVD cut has been cut and makes Smithers reply about not letting Lisa ahead of him make no sense.  One of my favorite things about the Simpsons was that they didn't care who they offended.  Well they went PC and now I'm highly offended!  If they hadn't messed with it I would give them 5 stars, or more realistically I wouldn't have bothered to write this review.  Quit tampering with the show, FOX!!\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6NyMZ5E4-QxM"
   },
   "source": [
    "<div style=\"padding:15px 20px 20px 20px;border-left:3px solid orange;background-color:#fff5d6;border-radius: 20px;color:#424242;\">\n",
    "\n",
    "# PART 2: Identify Model Shortcuts (22 pts)\n",
    "\n",
    "In this part, We aim to find out the shortcut features learnt by the sentiment analysis model we have trained in Part1. We will be using the model checkpoint trained with `learning rate=1e-5`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4lCHLdaH_3vT"
   },
   "source": [
    "## 🎯 Q2.1: **N-gram Pattern Extraction (6 pts)**\n",
    "We hypothesize that `n-gram`s could be the potential shortcut features learnt by the SA model. An `n-gram` is defined as a sequence of n consecutive words appeared in a natural language sentence or paragraph. \n",
    "\n",
    "Thus, we aim to extract that an n-gram that appears in a review may serve as a key indicator of the polarity of the review's sentiment, for example:\n",
    "\n",
    ">- **Review 1**: This book was **horrible**. If it was possible to rate it **lower than one star** I would have.\n",
    ">- **Review 2**: **Excellent** book, **highly recommended**. Helps to put a realistic perspective on millionaires.\n",
    "\n",
    "For Review 1, the `1-gram \"horrible\"` and the `4-gram \"lower than one star\"` serve as two key indicators of negative sentiment. While for Review 2, the `1-gram \"excellent\"` and the `2-gram \"highly recommended\"` obviously indicate positive sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_NovYRxv_3vU"
   },
   "source": [
    "TODO🔻: (1) [coding question] Complete `ngram_extraction()` function in `shortcut.py` file.\n",
    "\n",
    "The returned *ngrams* contains a **list** of dictionaries. The `n-th` **dictionary** corresponds the `n-grams` (n=1,2, 3, 4).\n",
    "\n",
    "The keys of each dictionary should be a **unique n-gram string** appeared in reviews, and the value of each n-gram key records the frequency of positive/negative predictions **made by the model** when the n-gram appears in the review, i.e., `\\[#positive_predictions, #negative_predictions\\]`.\n",
    "\n",
    "> Example: **`ngrams`[0]['horrible'][0]** should return the number of the positive predictions made by the model when the 1-gram token 'horrible' appear in the given review. i.e., \\[#positive_predictions, #negative_predictions\\].\n",
    "\n",
    "**Note:** (1) All the sequences contain punctuations should NOT be counted as a n-gram (e.g. `it is great .` is NOT a 4-gram, but `it is great` is a 3-gram); (2) All stop-words should NOT be counted as 1-grams, but can appear in other n-gram sequences (e.g. `is` is NOT a 1-gram token, but `it is great` can be a 3-gram token.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UTHt1frZ_3vU"
   },
   "source": [
    "## 🎯 Q2.2: **Distill Potentially Useful Patterns (8 pts)**\n",
    "\n",
    "TODO🔻: (2) [coding question] For each group of n-grams (n=1,2,3,4), find and **print** the **top-100 n-gram sequences** with the **greatest frequency of appearance**, which could contain frequent semantic features and would be used as our feature list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "IFZm6tFJFmlg"
   },
   "outputs": [],
   "source": [
    "from shortcut import ngram_extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "TUyal5mW_3vU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600/1600 [00:03<00:00, 446.27it/s]\n",
      "100%|██████████| 1600/1600 [00:03<00:00, 486.47it/s]\n",
      "100%|██████████| 1600/1600 [00:02<00:00, 788.84it/s]\n",
      "100%|██████████| 1600/1600 [00:01<00:00, 957.96it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-100 most frequent 1-grams:\n",
      "[('one', [2083, 1904]), ('book', [1895, 1856]), ('like', [1257, 1305]), ('would', [902, 1316]), ('good', [1146, 984]), ('movie', [919, 1002]), ('get', [872, 1035]), ('time', [986, 921]), ('great', [1336, 538]), ('well', [1085, 634]), ('even', [714, 878]), ('use', [913, 613]), ('much', [722, 781]), ('film', [797, 623]), ('really', [711, 701]), ('first', [707, 699]), ('also', [857, 527]), ('read', [688, 672]), ('j', [740, 540]), ('way', [570, 576]), ('work', [550, 595]), ('many', [618, 480]), ('better', [484, 582]), ('could', [458, 606]), ('k', [686, 374]), ('new', [552, 484]), ('two', [534, 500]), ('b', [559, 466]), ('people', [525, 497]), ('2', [487, 532]), ('make', [489, 523]), ('l', [615, 363]), ('little', [556, 417]), ('back', [409, 562]), ('story', [532, 430]), ('see', [524, 433]), ('r', [541, 407]), ('vd', [507, 441]), ('love', [696, 251]), ('g', [544, 393]), ('man', [526, 392]), ('think', [390, 516]), ('buy', [363, 541]), ('h', [465, 415]), ('never', [443, 432]), ('us', [522, 345]), ('w', [486, 379]), ('know', [423, 439]), ('best', [603, 258]), ('still', [480, 379]), ('c', [477, 366]), ('years', [455, 382]), ('p', [441, 387]), ('used', [445, 365]), ('bought', [359, 448]), ('3', [380, 423]), ('want', [395, 387]), ('life', [451, 316]), ('made', [364, 401]), ('go', [399, 364]), ('product', [327, 436]), ('set', [426, 334]), ('another', [351, 408]), ('quality', [430, 325]), ('got', [330, 413]), ('find', [372, 363]), ('1', [340, 372]), ('ing', [397, 315]), ('e', [373, 333]), ('thing', [320, 377]), ('n', [415, 281]), ('found', [310, 375]), ('say', [327, 355]), ('ever', [374, 308]), ('end', [332, 344]), ('bad', [197, 473]), ('every', [345, 322]), ('able', [340, 323]), ('old', [344, 309]), ('right', [371, 277]), ('something', [269, 370]), ('going', [289, 347]), ('since', [335, 301]), ('v', [360, 271]), ('need', [378, 251]), ('sound', [384, 242]), ('money', [213, 413]), ('5', [323, 302]), ('er', [317, 308]), ('easy', [509, 106]), ('f', [319, 294]), ('long', [329, 278]), ('books', [288, 317]), ('however', [293, 310]), ('works', [380, 206]), ('times', [285, 300]), ('put', [301, 283]), ('ed', [311, 271]), ('without', [305, 275]), ('characters', [298, 282])]\n",
      "Top-100 most frequent 2-grams:\n",
      "[('qu ot', [250, 156]), ('ip od', [143, 150]), ('much better', [67, 94]), ('christ ian', [66, 84]), ('even though', [72, 67]), ('highly recommend', [118, 15]), ('years ago', [70, 61]), ('first time', [61, 64]), ('j apan', [69, 55]), ('euro pe', [98, 25]), ('sound quality', [75, 43]), ('dish washer', [74, 43]), ('christ mas', [65, 50]), ('h ollywood', [55, 58]), ('mp 3', [69, 44]), ('every time', [49, 50]), ('customer service', [20, 79]), ('b rit', [54, 41]), ('g erman', [65, 28]), ('g ps', [64, 29]), ('kitchen aid', [38, 52]), ('would recommend', [61, 28]), ('j ames', [48, 40]), ('year old', [49, 38]), ('p eter', [46, 40]), ('cu isin', [22, 64]), ('isin art', [22, 63]), ('j es', [53, 31]), ('v ds', [52, 31]), ('es us', [51, 31]), ('apan ese', [48, 32]), ('long time', [56, 24]), ('5 stars', [47, 28]), ('new one', [25, 48]), ('ever seen', [37, 35]), ('dis ney', [17, 54]), ('ro bert', [40, 29]), ('pretty good', [37, 32]), ('ge orge', [37, 30]), ('vd player', [39, 27]), ('coffee maker', [20, 44]), ('several times', [32, 31]), ('works great', [50, 13]), ('another one', [23, 39]), ('ice cream', [21, 41]), ('log itech', [31, 31]), ('p aul', [29, 32]), ('really like', [38, 22]), ('je ff', [37, 23]), ('v iet', [49, 11]), ('pretty much', [27, 33]), ('ch inese', [38, 22]), ('one thing', [35, 24]), ('ear b', [30, 27]), ('pan asonic', [24, 33]), ('rit ish', [37, 19]), ('well worth', [51, 5]), ('ear phones', [31, 24]), ('step hen', [33, 21]), ('sm ith', [26, 28]), ('gar min', [33, 21]), ('rich ard', [29, 24]), ('make sure', [28, 25]), ('cal phal', [33, 20]), ('seems like', [20, 32]), ('feel like', [29, 23]), ('many times', [30, 21]), ('highly recommended', [49, 2]), ('works well', [42, 9]), ('first one', [23, 28]), ('cook ware', [39, 12]), ('har ry', [32, 18]), ('high school', [24, 26]), ('k agan', [50, 0]), ('looks like', [15, 35]), ('gr inder', [22, 28]), ('cord less', [30, 20]), ('non stick', [32, 18]), ('fl imsy', [11, 38]), ('buy another', [17, 32]), ('links ys', [18, 31]), ('3 player', [27, 22]), ('stainless steel', [31, 18]), ('many people', [31, 17]), ('mar x', [24, 24]), ('jack son', [29, 19]), ('little bit', [30, 18]), ('good job', [27, 21]), ('z en', [26, 22]), ('would like', [20, 27]), ('look like', [19, 27]), ('high quality', [29, 17]), ('b uds', [27, 19]), ('mo ore', [22, 23]), ('something else', [9, 36]), ('th omas', [26, 19]), ('ch ina', [19, 26]), ('law rence', [20, 25]), ('eng land', [33, 12]), ('le e', [26, 18])]\n",
      "Top-100 most frequent 3-grams:\n",
      "[('cu isin art', [22, 63]), ('j es us', [51, 31]), ('j apan ese', [48, 32]), ('b rit ish', [37, 19]), ('mp 3 player', [27, 22]), ('ear b uds', [26, 19]), ('ro ber ts', [16, 23]), ('christ ian ity', [14, 19]), ('w ides creen', [16, 17]), ('j ose ph', [15, 16]), ('ut ens ils', [18, 13]), ('v iet nam', [24, 7]), ('r us sell', [27, 3]), ('b erg man', [30, 0]), ('would highly recommend', [24, 5]), ('k enn edy', [15, 13]), ('j enn ifer', [12, 15]), ('w w ii', [11, 15]), ('step hen king', [14, 12]), ('k ru ps', [5, 20]), ('far ber ware', [14, 9]), ('n ich olas', [19, 3]), ('je ff erson', [6, 16]), ('diss ap ointed', [5, 16]), ('j ess ica', [6, 15]), ('le b owski', [20, 1]), ('ge h ry', [0, 21]), ('sub wo ofer', [9, 12]), ('coff eem aker', [13, 8]), ('nc ke ls', [7, 14]), ('ham ilton beach', [13, 8]), ('c ind erella', [19, 1]), ('k l ips', [16, 4]), ('l ips ch', [16, 4]), ('b org es', [5, 14]), ('rec om end', [10, 9]), ('core l le', [14, 5]), ('20 th century', [9, 9]), ('ko ont z', [0, 18]), ('h oo ver', [2, 16]), ('f iest aware', [12, 6]), ('san franc isco', [11, 6]), ('cc om end', [11, 6]), ('buy another one', [8, 9]), ('ch ris ben', [15, 2]), ('bon hoe ffer', [17, 0]), ('es us christ', [7, 9]), ('ver bat im', [10, 6]), ('k ens ington', [7, 9]), ('adam sand ler', [10, 6]), ('le cre us', [10, 6]), ('cre us et', [10, 6]), ('worth every penny', [13, 2]), ('ris ben oit', [14, 1]), ('noise cance lling', [7, 8]), ('view son ic', [14, 1]), ('world war ii', [10, 4]), ('g ps unit', [12, 2]), ('dish washer safe', [11, 3]), ('euro pe ans', [12, 1]), ('g oth ic', [11, 2]), ('j ul ie', [9, 4]), ('bec u ase', [6, 7]), ('new eng land', [12, 1]), ('bet te mid', [12, 1]), ('te mid ler', [12, 1]), ('creative z en', [6, 7]), ('cro ck pot', [6, 7]), ('uncon v inc', [3, 9]), ('se pt ember', [8, 4]), ('eng ross ing', [9, 3]), ('many years ago', [9, 3]), ('p ix ar', [1, 11]), ('rat l iff', [12, 0]), ('ben oit vs', [12, 0]), ('ear b ud', [4, 8]), ('sk il lets', [8, 4]), ('v inc ing', [3, 8]), ('really looking forward', [2, 9]), ('ins ip id', [0, 11]), ('mar x ist', [9, 2]), ('los angel es', [9, 2]), ('j ol ie', [4, 7]), ('cr ich ton', [11, 0]), ('h ann ibal', [3, 8]), ('cam c order', [5, 6]), ('k ief er', [4, 7]), ('p au ly', [10, 1]), ('ver hoe ven', [11, 0]), ('je ff ery', [11, 0]), ('cord less phone', [5, 6]), ('z en micro', [8, 3]), ('pal est inian', [10, 0]), ('mel od rama', [5, 5]), ('har ry pot', [5, 5]), ('ry pot ter', [5, 5]), ('dis j ointed', [2, 8]), ('cl ic hes', [4, 6]), ('gr ac ie', [0, 10]), ('e hr man', [4, 6])]\n",
      "Top-100 most frequent 4-grams:\n",
      "[('k l ips ch', [16, 4]), ('j es us christ', [7, 9]), ('le cre us et', [10, 6]), ('ch ris ben oit', [14, 1]), ('bet te mid ler', [12, 1]), ('ris ben oit vs', [12, 0]), ('uncon v inc ing', [3, 8]), ('har ry pot ter', [5, 5]), ('l ily tom lin', [10, 0]), ('je ff ery le', [10, 0]), ('ff ery le b', [10, 0]), ('ery le b owski', [10, 0]), ('w ither sp oon', [4, 5]), ('cl int east wood', [5, 4]), ('p au ly shore', [8, 1]), ('10 ch ris ben', [9, 0]), ('al tec l ans', [8, 1]), ('h ong k ong', [6, 2]), ('f aul k ner', [6, 2]), ('j ose ph sm', [1, 7]), ('ose ph sm ith', [1, 7]), ('k en r us', [8, 0]), ('en r us sell', [8, 0]), ('sand ra bull ock', [5, 3]), ('j ess ica l', [2, 6]), ('ess ica l ange', [2, 6]), ('john ny de pp', [5, 3]), ('jam ie le e', [2, 6]), ('f red ast aire', [7, 1]), ('sad ie shel ton', [8, 0]), ('tec l ans ing', [7, 1]), ('z oj ir ushi', [6, 2]), ('n vidia quad ro', [8, 0]), ('vidia quad ro fx', [8, 0]), ('un sy mp athetic', [2, 5]), ('dean ko ont z', [0, 7]), ('w alt dis ney', [2, 5]), ('th omas je ff', [2, 5]), ('omas je ff erson', [2, 5]), ('p aul new man', [3, 4]), ('k uy k end', [7, 0]), ('n ich olas nick', [7, 0]), ('ich olas nick le', [7, 0]), ('r ask oln ik', [7, 0]), ('ask oln ik ov', [7, 0]), ('j ulia ro ber', [1, 6]), ('ulia ro ber ts', [1, 6]), ('p fe iff er', [5, 2]), ('ro bert red ford', [0, 7]), ('gl enda jack son', [7, 0]), ('w uther ing heights', [6, 1]), ('poly com commun icator', [7, 0]), ('n vidia g ef', [7, 0]), ('vidia g ef orce', [7, 0]), ('metal ut ens ils', [5, 2]), ('angel ina j ol', [2, 4]), ('ina j ol ie', [2, 4]), ('bet ty cro cker', [5, 1]), ('ter ry g ill', [4, 2]), ('ry g ill iam', [4, 2]), ('v iet nam war', [5, 1]), ('jo e pick ett', [4, 2]), ('chan z k ows', [6, 0]), ('z k ows ka', [6, 0]), ('ind iana j ones', [5, 1]), ('den zel washing ton', [3, 3]), ('ge orge clo oney', [0, 6]), ('cl ive ow en', [0, 6]), ('ie le e cur', [2, 4]), ('le e cur tis', [2, 4]), ('pat ty hear st', [6, 0]), ('frank ge h ry', [0, 6]), ('em ile hen ry', [3, 3]), ('l ud l um', [3, 2]), ('ad olf hit ler', [3, 2]), ('j ames pat terson', [1, 4]), ('f uk uy ama', [4, 1]), ('uns oph istic ated', [1, 4]), ('step hen j ay', [0, 5]), ('hen j ay g', [0, 5]), ('j ay g ould', [0, 5]), ('h uckle berry fin', [5, 0]), ('uckle berry fin n', [5, 0]), ('j apan ese fleet', [5, 0]), ('au w ied er', [5, 0]), ('w ied er se', [5, 0]), ('ied er se hen', [5, 0]), ('p eter jack son', [2, 3]), ('ai ji j ian', [5, 0]), ('mic key ro oney', [3, 2]), ('j apan ese version', [4, 1]), ('matt hew bro der', [3, 2]), ('j ess ica sim', [0, 5]), ('ess ica sim pson', [0, 5]), ('big le b owski', [4, 1]), ('k arl john son', [0, 5]), ('anth ony hop kins', [2, 3]), ('k enn edy assassination', [0, 5]), ('j ul ian ne', [4, 1]), ('ul ian ne mo', [4, 1])]\n"
     ]
    }
   ],
   "source": [
    "# all your saved model prediction results from 1.3 Fine-Grained Validation\n",
    "prediction_files = [\"predictions/test_\"+domain+\".jsonl\" for domain in domain_data.keys()]\n",
    "\n",
    "# TODO: Define your tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"models/lr1e-05-warmup0.3\")\n",
    "ngrams = ngram_extraction(prediction_files, tokenizer)\n",
    "\n",
    "top_100 = {}\n",
    "for n, counts in enumerate(ngrams):\n",
    "    # TODO: find top-100 n-grams (n=1,2,3 or 4) associated with the greatest frequency of appearance\n",
    "    top_100_freq = sorted(counts.items(), key=lambda x: x[1][0] + x[1][1], reverse=True)[:100]\n",
    "\n",
    "    print(f'Top-100 most frequent {n+1}-grams:')\n",
    "    print(top_100_freq)\n",
    "\n",
    "    top_100[n] = top_100_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pz25EvuI_3vU"
   },
   "source": [
    "**Among each type of top-100 frequent n-grams above**, we aim to further find out the n-grams which **most likely** lead to *positive*/*negative* predictions (positive/negative shortcut features). \n",
    "\n",
    "TODO🔻: (3) [coding&text question] Design **two different methods to re-rank** the top-100 n-grams to extract shortcut features. For each method, you should extract **1** feature in each of n-grams group (n=1, 2, 3, 4) for positve and negative prediction (1\\*4\\*2=8 features in total for 1 method).\n",
    "\n",
    "Explain each of your design choices in natural language, and compare which method finds more reasonable patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Yq2cVOaWTEYw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 positive patterns:\n",
      "['great', 'k agan', 'b erg man', 'ris ben oit vs']\n",
      "Top-1 negative patterns:\n",
      "['book', 'customer service', 'ge h ry', 'dean ko ont z']\n"
     ]
    }
   ],
   "source": [
    "# TODO: [Method 1] find top-1 positive and negative patterns\n",
    "top_1 = [[{}, {}, {}, {}], [{}, {}, {}, {}]]\n",
    "scores = [[{}, {}, {}, {}], [{}, {}, {}, {}]]\n",
    "for n, occurrences in top_100.items():\n",
    "    for ngram, (pos, neg) in occurrences:\n",
    "        pos_score = (pos + neg) * (pos / (neg + 1))\n",
    "        neg_score = (pos + neg) * (neg / (pos + 1))\n",
    "        scores[0][n][ngram] = pos_score\n",
    "        scores[1][n][ngram] = neg_score\n",
    "\n",
    "for n in range(len(top_100)):\n",
    "    top_1[0][n] = max(scores[0][n], key=scores[0][n].get)\n",
    "    top_1[1][n] = max(scores[1][n], key=scores[1][n].get)\n",
    "\n",
    "print(f'Top-1 positive patterns:')\n",
    "print(top_1[0])\n",
    "print(f'Top-1 negative patterns:')\n",
    "print(top_1[1])\n",
    "\n",
    "# TODO: [Explanation of Method 1]\n",
    "# I used the formula (total occurence) * ratio of positive occurence to negative occurence\n",
    "# to calculate the positive score for each n-gram. (and vice versa for negative score) To support\n",
    "# the formula, I added 1 to the denominator to avoid division by zero.\n",
    "# As an analysis, only the top-1 positive pattern for 1-gram give decent shortcut feature (great word for positive sentiment) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "Yq2cVOaWTEYw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 positive patterns:\n",
      "['great', 'highly recommend', 'b erg man', 'ris ben oit vs']\n",
      "Top-1 negative patterns:\n",
      "['would', 'customer service', 'cu isin art', 'dean ko ont z']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: [Method 2] find top-1 positive and negative patterns\n",
    "top_1 = [[{}, {}, {}, {}], [{}, {}, {}, {}]]\n",
    "scores = [[{}, {}, {}, {}], [{}, {}, {}, {}]]\n",
    "for n, occurrences in top_100.items():\n",
    "    for ngram, (pos, neg) in occurrences:\n",
    "        pos_score = (pos + neg) * np.log((pos + 1) / (neg + 1))\n",
    "        neg_score = (pos + neg) * np.log((neg + 1) / (pos + 1))\n",
    "        scores[0][n][ngram] = pos_score\n",
    "        scores[1][n][ngram] = neg_score\n",
    "\n",
    "for n in range(len(top_100)):\n",
    "    top_1[0][n] = max(scores[0][n], key=scores[0][n].get)\n",
    "    top_1[1][n] = max(scores[1][n], key=scores[1][n].get)\n",
    "\n",
    "print(f'Top-1 positive patterns:')\n",
    "print(top_1[0])\n",
    "print(f'Top-1 negative patterns:')\n",
    "print(top_1[1])\n",
    "\n",
    "# TODO: [Explanation of Method 2]\n",
    "# In the first method, I observed that, if positive or negative occurence of a word is very low or zero, it affects the score\n",
    "# significantly. To avoid this, I used the log function to calculate the score.\n",
    "# Score = (total occurence) * log(ratio of positive occurence to negative occurence) for positive score (and vice versa for negative score)\n",
    "# As an analysis, the top-1 positive pattern for 1-gram and 2-gram give decent shortcut features \n",
    "# ('great' word and 'highly recommend' words for positive sentiment) So, I would prefer the second method over the first one.\n",
    "# However, still the other features are not very useful for sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvKyF0UFuXXM"
   },
   "source": [
    "TODO🔻: Compare and discuss the results from two methods above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjVti-vL_3vV"
   },
   "source": [
    "## 🎯 Q2.3: **Case Study (8 pts)**\n",
    "\n",
    "TODO🔻: Among the shortcut features you found in 2.1, find out **4 representative** cases (pair of `\\[review, n-gram feature\\]`) where the shortcut feature **will lead to a wrong prediction**. \n",
    "\n",
    "For example, the 1-gram feature \"excellent\" has been considered as a shortcut for *positive* sentiment, while the ground-truth label of the given review containing \"excellent\" is *negative*.\n",
    "\n",
    "**Questions:**\n",
    "- Based on your case study, do you detect any limitations of the n-gram patterns?\n",
    "- Which type of n-gram (1/2/3/4-gram) pattern is more robust to be used for sentiment prediction shortcut and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "2XyPmb01_3vV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'review': \"When I bought this book, I didn't realize it was mostly just a consolidated writing of Friedman's collumns in the Times.  I think Friedman is a great author with lots of great insights, but he isn't able to go into his ideas in depth as much as I would have liked in a bunch of detached 750-1000 word segments.  Since the sections are arranged chronologically, there also isn't the opportunity to tie the themes together. If you really like Friedman, then it's worth a read, but if you are in it for just one, I'd read The Lexus and the Olive Tree first.  It is by far his best\", 'domain': 'books', 'label': 'negative'}\n",
      "{'review': 'The title is a misrepresentation.  This is not a handbook for driving a Porsche quickly or professionally.  I bought this book because in 2 months I am going to drive my rear engined 993 on a F1 circuit.  I have no circuit experience, and cannot get any coaching or any circuit experience in the country I live in.  Since this is a rear engined car which I have little experience with I was particularly wanting some tips particular to rear engined cars.  There is nothing in the whole book which is particular to driving Porsches or rear engined cars.  The only reason I will not throw this book out is because it has some interesting 1960\\'s and early 19970\\'s Porsche racing pictures (poor quality black and white though).  Also to be fair there are some good racing stories. He is undoubtedly a very impressive driver having been successful at the highest levels in rallying and sports prototype endurance racing.  A better title would be \"My Porsche Racing History\".  If you want a book on fast driving I would highly recommend Ross Bentley\\'s \"Speed Secrets\" in place of this book', 'domain': 'books', 'label': 'negative'}\n",
      "{'review': 'Being a dyed-in-the-wool Mcguane fan, I think he just keeps getting better.  Not only is he the best fishing writer alive, but his non-fishing stories are bizzarely entertaining, have an interesting twist, and always explore human interaction. I believe his writing is based on his many and varied experiences, from crazy street people to ego-crazed CEOs, as well as the many places he has lived.  Who else would know what a cabinet is, except a Rhode Island resident?  What...no mention of hot weiners or coffee milk?', 'domain': 'books', 'label': 'positive'}\n",
      "{'review': \"Maybe you had to be there.  Maybe the actual film never quite measures up to what has been preserved in the memory of a certain generation.  And with literary roots in journalistic observations of the blandly benign Cameron Crowe, maybe it lacks the nerve to truly explore the teenage sex blues with the farcical abandon of its vulgar cousin Porky's. But even today Fast Times remains funny, with witty set pieces and a host of emerging Hollywood stars flirting, dating and hooking up to the rock soundtrack of the period. As burnout surfer Jeff Spicoli, Sean Penn achieved well-deserved legendary status.  Yet special props go to Judge Reinhold as the hang-dog dirty old senior.  Any scene even remotely involving customer service, he steals it, then turns around and gives able dramatic support to Jennifer Jason Leigh\", 'domain': 'dvd', 'label': 'positive'}\n"
     ]
    }
   ],
   "source": [
    "# TODO: you can fill your code for finding cases here\n",
    "# 4 cases in total: 1-grams and 2-grams for positive and negative sentiments\n",
    "with jsonlines.open(\"data/test_sa.jsonl\", mode=\"r\") as reader:\n",
    "    for pattern in [top_1[0][0], top_1[0][1]]:\n",
    "        for sample in reader:\n",
    "            if sample[\"review\"].find(pattern) != -1 and sample[\"label\"] == \"negative\":\n",
    "                print(sample)\n",
    "                break\n",
    "    for pattern in [top_1[1][0], top_1[1][1]]:\n",
    "        for sample in reader:\n",
    "            if sample[\"review\"].find(pattern) != -1 and sample[\"label\"] == \"positive\":\n",
    "                print(sample)\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c45cvlUqufRR"
   },
   "source": [
    "TODO🔻: (Write your case study discussions and answers to the questions here.)\n",
    "\n",
    "**Answers:** \n",
    "\n",
    "- One of the most important limitations of n-gram patterns is that they are not able to identify the global context of the review. And since they cant get the global context, even reviewer telling \"great\" to another movie in the review, it seems as a positive review for n-gram patterns. Actually this results show the importance of getting word embedding of the words based on the context. In addition to this, it becomes harder to get frequencies exponentially when we increase the n-gram size. Therefore it can be hard to collect enough data to make a decision when we increase n (actually in our case we get nonsense patterns for 3-gram and 4-grams which support this claim).\n",
    "- 1-gram or 2-grams patterns can be more robust due to data sparsity issue in 3-grams and 4-grams. Since the 1-gram patterns are more frequent in the reviews, they can be more robust to be used for sentiment prediction shortcut. However, 2-grams can be more robust than 1-grams if there are enough samples because they can capture more context than 1-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yND0DEfT-eXn"
   },
   "source": [
    "<div style=\"padding:15px 20px 20px 20px;border-left:3px solid orange;background-color:#fff5d6;border-radius: 20px;color:#424242;\">\n",
    "\n",
    "## **Part 3: Annotate New Data (25 pts)**\n",
    "\n",
    "In this part, you will **annotate** the gold labels of some **new** SA data samples, and measure the degree of **agreement** between your and **one or two partners'** annotations.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQNXrRHr_3vV"
   },
   "source": [
    "## 🎯 Q3.1: **Write an Annotation Guideline (5 pts)**\n",
    "\n",
    "TODO🔻: Imagine that you are going to assign this annotation task to a crowdsourcing worker, who is completely not familiar with computer science and NLP. Think about how you are going to explain this annotation task to him in order to guide him do a decent job. Write an annotation guideline for such a worker who are going to do this task for you.\n",
    "\n",
    "**Note:** You should come up with your own guideline without the help of your partner(s) in later Part 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfkqNbUA_3vV"
   },
   "source": [
    "(Write your annotation guideline here.)\n",
    "\n",
    "**Annotation Guideline:**\n",
    "- **Description of the Task:** \n",
    "    - There are set of reviews about different products on 4 different domains, namely; books, dvd, electronics, and housewares.\n",
    "    - Your task is to read each review and decide/label whether emotional tone of the review is positive or negative. If the review is praising the product, or the reviewer is happy with the product, you should label it as positive.\n",
    "    - On the other hand, if the review is criticizing the product, or the reviewer is stating unhappinness about the product, you should label it as negative. You should consider the context of the review while deciding the sentiment. For instance, if the reviewer is criticizing the product but also praising some aspects of it and the tone is positive, you should consider the overall sentiment of the review.\n",
    "- **Review Examples:** You can see two example reviews below to understand the task better: (These are from domain 'books' as an example)\n",
    "    - Positive Review: \"The book arrived as expected and was in great shape.  Thanks\"\n",
    "    - Negative Review: \"This seemed too long and too drawn out\"\n",
    "- **Final Notes:** \n",
    "    - Please read the reviews carefully and try to understand the overall sentiment and emotion of the review. If you are not sure about the sentiment, please reread the review and try to understand the context better and label the review according to the overall emotion.\n",
    "    - We will use your annotations to train an accurate model to predict the sentiment/emotion of the reviews, so please be careful while annotating the reviews.\n",
    "    - If you have any questions or need help, feel free to ask.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XBWK4Bw__3vV"
   },
   "source": [
    "## 🎯 Q3.2: **Annotate Your Datapoints with Partner(s) (8 pts)**\n",
    "\n",
    "TODO🔻: Annotate 80 datapoints (20 in each domain of \"books\", \"dvd\", \"electronics\" and \"housewares\") assigned to you and your partner(s), by editing the value of the key **\"label\"** in each datapoint. You and your partner(s) should annotate **independently of each other**, i.e., each of you provide your own 80 annotations.\n",
    "\n",
    "Please find your assigned annotation dataset **ID** and **your partner(s)** according to this [list](https://docs.google.com/spreadsheets/d/1hOwBUb8XE8fitYa4hlAwq8mARZe3ZsL4/edit?usp=sharing&ouid=108194779329215429936&rtpof=true&sd=true). Your annotation dataset can be found [here](https://drive.google.com/drive/folders/1IHXU_v3PDGbZG6r9T5LdjKJkHQ351Mb4?usp=sharing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWhjTn2fQ5YE"
   },
   "source": [
    "**Name your annotated file as `<your_assigned_dataset_id>-<your_sciper_number>.jsonl`.**\n",
    "\n",
    "**You should also submit your partner's annotated file `<assigned_dataset_id>-<your_partner_sciper_number>.jsonl`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT NOTE: I contacted with my partner but could not get any response in 1 week. Therefore, I contacted with other person with same dataset id assigned (Pablo Nicolas Soto Gomez (383334)) and used his annotations for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uyP0GtHe_3vW"
   },
   "source": [
    "## 🎯 Q3.3: **Agreement Measure (12 pts)**\n",
    "\n",
    "TODO🔻: Based on your and your partner's annotations in 3.2, calculate the [Cohen's Kappa](https://scikit-learn.org/stable/modules/model_evaluation.html#cohen-kappa) or [Krippendorff's Alpha](https://github.com/pln-fing-udelar/fast-krippendorff) (if you are in a group of three students) between the annotators on **each domain** and **across all domains**.\n",
    "\n",
    "**Note:** Cohen's Kappa or Krippendorff's Alpha interpretation\n",
    "\n",
    "0: No Agreement\n",
    "\n",
    "0 ~ 0.2: Slight Agreement\n",
    "\n",
    "0.2 ~ 0.4: Fair Agreement\n",
    "\n",
    "0.4 ~ 0.6: Moderate Agreement\n",
    "\n",
    "0.6 ~ 0.8: Substantial Agreement\n",
    "\n",
    "0.8 ~ 1.0: Near Perfect Agreement\n",
    "\n",
    "1.0: Perfect Agreement\n",
    "\n",
    "**Questions:**\n",
    "- What is the overall degree of agreement between you and your partner(s) according to the above interpretation of score ranges?\n",
    "- In which domain are disagreements most and least frequently happen between you and your partner(s)? Give some examples to explain why that is the case.\n",
    "- Are there possible ways to address the disagreements between annotators?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5_2VlClO38Jt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agreement score: 0.8439024390243902\n"
     ]
    }
   ],
   "source": [
    "# Fill your code for calculating agreement scores here.\n",
    "my_labels = []\n",
    "partner_labels = []\n",
    "\n",
    "with jsonlines.open(\"data/53-369141.jsonl\", mode=\"r\") as reader:\n",
    "    for sample in reader:\n",
    "        my_labels.append(sample[\"label\"])\n",
    "\n",
    "with jsonlines.open(\"data/53-383334.jsonl\", mode=\"r\") as reader:\n",
    "    for sample in reader:\n",
    "        partner_labels.append(sample[\"label\"])\n",
    "\n",
    "print(f'Agreement score: {cohen_kappa_score(my_labels, partner_labels)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I98NGWYb3zl8"
   },
   "source": [
    "(Write your answers to the questions here.)\n",
    "\n",
    "**Answers:**\n",
    "- The overall degree of agreement between me and my partner is near perfect with a Cohen's Kappa score of ~0.844.\n",
    "- Our disagreements happen 1 in dvd, 1 in electronics and 2 in housewares domains. The main reason for the disagreements is the ambiguity of the reviews. For instance, in the dvd domain, in 28th review, the reviewer first mention that \"I still love this movie, so I'm giving it 4 stars\" and also mention \"Actor's voices have lost the human richness\" and \"Even the music and sound effects sound a little \"off.\"\". So, both positive and negative emotion and  semantic occurs in the same review which makes it hard to label. Another reason for disagreement in 2 housewares domain was, the reviewer first mention the other reviews and then mention his/her perspective which can be confusing for annotators if they dont give enough attention to the context. \n",
    "- The possible way to address the disagreements are to provide more detailed annotation guidelines and examples to the annotators. Also, when I checked the agreements I observed that most of the disagreements are due to the ambiguity of the reviews. Therefore, it would be better to decide annotating a review solely on its emotion or sentiment, or language style."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4wuRpHt-rQF"
   },
   "source": [
    "<div style=\"padding:15px 20px 20px 20px;border-left:3px solid orange;background-color:#fff5d6;border-radius: 20px;color:#424242;\">\n",
    "\n",
    "## **Part 4: Data Augmentation (20 pts)**\n",
    "\n",
    "Since we only used 20% of the whole dataset for training, which might limit the model performance. In the final part, we will try to enlarge the training set by **data augmentation**.  \n",
    "\n",
    "Specifically, we will **`Rephrase`** some current training samples using pretrained paraphraser. So that the paraphrased synthetic samples would preserve the semantic similarity while change the surface format.\n",
    "\n",
    "You can use the pretrained T5 paraphraser [here](https://huggingface.co/humarin/chatgpt_paraphraser_on_T5_base).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQa2q1io_5Pk"
   },
   "source": [
    "## 🎯 Q4.1: **Data Augmentation with Paraphrasing (15 pts)**\n",
    "TODO🔻: Implement functions named `get_paraphrase_batch` and `get_paraphrase_dataset` with the details in the below two blocks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "nTMdZ-azABk-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saidgurbuz/miniconda3/envs/mnlp-a2/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# get the given pretrained paraphrase model and the corresponding tokenizer (https://huggingface.co/humarin/chatgpt_paraphraser_on_T5_base)\n",
    "paraphrase_tokenizer = AutoTokenizer.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\")\n",
    "paraphrase_model = AutoModelForSeq2SeqLM.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\").to(device)\n",
    "\n",
    "def get_paraphrase_batch(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    input_samples,\n",
    "    n,\n",
    "    repetition_penalty=10.0,\n",
    "    diversity_penalty=3.0,\n",
    "    no_repeat_ngram_size=2,\n",
    "    temperature=0.7,\n",
    "    max_length=256,\n",
    "    device='mps'):\n",
    "    '''\n",
    "    Input\n",
    "      model: paraphraser\n",
    "      tokenizer: paraphrase tokenizer\n",
    "      input_samples: a batch (list) of real samples to be paraphrased\n",
    "      n: number of paraphrases to get for each input sample\n",
    "      for other parameters, please refer to:\n",
    "          https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.GenerationConfig\n",
    "    Output: Tuple.\n",
    "      synthetic_samples: a list of paraphrased samples\n",
    "    '''\n",
    "\n",
    "    # TODO: implement paraphrasing on a batch of imput samples\n",
    "    synthetic_samples = []\n",
    "    inputs = tokenizer([sample['review'] for sample in input_samples], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "      outputs = model.generate(**inputs, repetition_penalty=repetition_penalty, diversity_penalty=diversity_penalty, no_repeat_ngram_size=no_repeat_ngram_size, temperature=temperature, max_length=max_length, num_return_sequences=n, num_beams=n, num_beam_groups=n)\n",
    "    \n",
    "    for i, sample in enumerate(input_samples):\n",
    "      paraphrases = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs[i * n : (i + 1) * n]]\n",
    "      synthetic_samples.extend([{'review': paraphrase, 'domain': sample['domain'], 'label': sample['label']} for paraphrase in paraphrases])\n",
    "\n",
    "    return synthetic_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "aL29QCAodF1b"
   },
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "\n",
    "data_dir = 'data'\n",
    "data_train_path = os.path.join(data_dir, 'train_sa.jsonl')\n",
    "BATCH_SIZE = 4\n",
    "N_PARAPHRASE = 2\n",
    "\n",
    "def get_paraphrase_dataset(model, tokenizer, data_path, batch_size, n_paraphrase):\n",
    "    '''\n",
    "    Input\n",
    "      model: paraphrase model\n",
    "      tokenizer: paraphrase tokenizer\n",
    "      data_path: path to the `jsonl` file of training data\n",
    "      batch_size: number of input samples to be paraphrases in one batch\n",
    "      n_paraphrase: number of paraphrased sequences for each sample\n",
    "    Output:\n",
    "      paraphrase_dataset: a list of all paraphrase samples. Do not include the original training data.\n",
    "    '''\n",
    "    paraphrase_dataset = []\n",
    "    with jsonlines.open(data_path, \"r\") as reader:\n",
    "\n",
    "        # TODO: get paraphrases for the whole training dataset using get_paraphrase_batch\n",
    "        samples = []\n",
    "        for sample in reader:\n",
    "            samples.append(sample)\n",
    "            if len(samples) == batch_size:\n",
    "                print(f\"Getting paraphrases for {len(paraphrase_dataset) + len(samples)} samples\")\n",
    "                paraphrase_dataset.extend(get_paraphrase_batch(model, tokenizer, samples, n_paraphrase))\n",
    "                samples = []\n",
    "        \n",
    "        if len(samples) > 0:\n",
    "            paraphrase_dataset.extend(get_paraphrase_batch(model, tokenizer, samples, n_paraphrase))\n",
    "\n",
    "    return paraphrase_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** run paraphrasing, which will take ~20-30 minutes using a T4 Colab GPU. But the running time could depend on various implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paraphrase_dataset = get_paraphrase_dataset(paraphrase_model, paraphrase_tokenizer, data_train_path, BATCH_SIZE, N_PARAPHRASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "nGQsvD4dktVv"
   },
   "outputs": [],
   "source": [
    "# Original training dataset\n",
    "with jsonlines.open(data_train_path, \"r\") as reader:\n",
    "    origin_data = [dt for dt in reader.iter()]\n",
    "\n",
    "all_data = origin_data + paraphrase_dataset\n",
    "\n",
    "# Write all the original and paraphrased data samples into training dataset\n",
    "augmented_data_train_path = os.path.join(data_dir, 'augmented_train_sa.jsonl')\n",
    "with jsonlines.open(augmented_data_train_path, \"w\") as writer:\n",
    "    writer.write_all(all_data)\n",
    "\n",
    "assert len(all_data) == 3 * len(origin_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PmFpfxrjWA1O"
   },
   "source": [
    "## 🎯 Q4.2: **Retrain RoBERTa Model with Data Augmentation (5 pts)** \n",
    "TODO🔻: Retrain the sentiment analysis model with the augmented (original+paraphrased), larger dataset :)\n",
    "\n",
    "**Note:** *Training on the augmented data will take about 15 minutes using a T4 Colab GPU.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "sz9gQSe8ANix"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building SA Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4800it [00:01, 3904.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building SA Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6400it [00:02, 2678.75it/s]\n",
      "/Users/saidgurbuz/miniconda3/envs/mnlp-a2/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Training:   0%|          | 0/600 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "Training: 100%|██████████| 600/600 [02:30<00:00,  3.99it/s]\n",
      "Evaluation: 100%|██████████| 800/800 [00:46<00:00, 17.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Training Loss: 0.530 | Validation Loss: 0.302\n",
      "Epoch 0 SA Validation:\n",
      "Confusion Matrix:\n",
      "[[2930  270]\n",
      " [ 413 2787]]\n",
      "F1: (89.56%, 89.08%) | Macro-F1: 89.32%\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 600/600 [02:17<00:00,  4.37it/s]\n",
      "Evaluation: 100%|██████████| 800/800 [00:46<00:00, 17.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Training Loss: 0.347 | Validation Loss: 0.306\n",
      "Epoch 1 SA Validation:\n",
      "Confusion Matrix:\n",
      "[[2854  346]\n",
      " [ 311 2889]]\n",
      "F1: (89.68%, 89.79%) | Macro-F1: 89.73%\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 600/600 [02:16<00:00,  4.38it/s]\n",
      "Evaluation: 100%|██████████| 800/800 [00:46<00:00, 17.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Training Loss: 0.195 | Validation Loss: 0.588\n",
      "Epoch 2 SA Validation:\n",
      "Confusion Matrix:\n",
      "[[2729  471]\n",
      " [ 212 2988]]\n",
      "F1: (88.88%, 89.74%) | Macro-F1: 89.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 600/600 [02:16<00:00,  4.39it/s]\n",
      "Evaluation: 100%|██████████| 800/800 [00:46<00:00, 17.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Training Loss: 0.104 | Validation Loss: 0.555\n",
      "Epoch 3 SA Validation:\n",
      "Confusion Matrix:\n",
      "[[2888  312]\n",
      " [ 306 2894]]\n",
      "F1: (90.33%, 90.35%) | Macro-F1: 90.34%\n",
      "Model Saved!\n"
     ]
    }
   ],
   "source": [
    "# Re-train a RoBERTa SA model on the augmented training dataset\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "\n",
    "model_name = \"FacebookAI/roberta-base\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "batch_size = 8\n",
    "epochs = 4\n",
    "max_grad_norm = 1.0\n",
    "warmup_percent = 0.3\n",
    "learning_rate = 1e-5\n",
    "\n",
    "train_dataset = SADataset(\"data/augmented_train_sa.jsonl\", tokenizer)\n",
    "test_dataset = SADataset(\"data/test_sa.jsonl\", tokenizer)\n",
    "\n",
    "train(train_dataset, test_dataset, model, device, batch_size=batch_size, epochs=epochs, max_grad_norm=max_grad_norm, warmup_percent=warmup_percent, learning_rate=learning_rate,\n",
    "      model_save_root='models/augmented/', tensorboard_path=\"./tensorboard/part4_lr{}\".format(learning_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1PRvIB7ZAyaE"
   },
   "source": [
    "TODO🔻: Discuss your results by answering the following questions\n",
    "\n",
    "- Compare the performances of models in Part 1 and Part 4. Does the data augmentation help with the performance and why (give possible reasons)?\n",
    "- No matter whether the data augmentation helps or not, list **three** possible ways to improve our current data augmentation method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5-dVSsniH9N"
   },
   "source": [
    "(Write your answers to the questions here.)\n",
    "\n",
    "**Answers:**\n",
    "- The performances after data augmentation is not better and even slightly worse than the model trained on the original data. This can be due to the quality of the paraphrases generated by the model. Some of the paraphrases generated by the model are not complete sentences or do not make sense. And I observed that, most of the paraphrases' lengths are much shorter than the original reviews even though I set large max_length argument which causes the model to generate incomplete paraphrases sometimes. This can be the reason why the model trained on the augmented data performed worse than the model trained on the original data. One possible reason for getting slightly worse results can be related to not using the best hyperparameters for the model training after data augmentation.(So, actually comparing the results after finding best hyperparameters configurations (e.g. with grid-search) would be more accurate to decide if data augmentation helps or not)\n",
    "- Three possible ways to improve our current data augmentation method are:\n",
    "    - Using more advanced paraphrasing models that can generate more diverse, complete, and longer paraphrases.\n",
    "    - Finetuning the paraphrasing model on the specific domain data to generate more domain-specific paraphrases which can provide semantically/emotionally more accurate paraphrases.\n",
    "    - Using different hyperparameter configurations for the model which is optimized for the augmented data. (such as smaller learning rate as we have more data now) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3CIeN_kaOQl"
   },
   "source": [
    "<div style=\"padding:15px 20px 20px 20px;border-left:3px solid orange;background-color:#fff5d6;border-radius: 20px;color:#424242;\">\n",
    "\n",
    "### **5 Upload Your Notebook, Data and Models**\n",
    "\n",
    "Please upload your filled jupyter notebook in your GitHub Classroom repository, **with all cells run and output results shown**.\n",
    "\n",
    "**Note:** We are **not** responsible for re-running the cells in your notebook.\n",
    "\n",
    "Please also submit all your **datasets** **(anotated and augmented)**, as well as **all your trained models** in Part 1 and Part 4, in your GitHub Classroom repository.\n",
    "    \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
